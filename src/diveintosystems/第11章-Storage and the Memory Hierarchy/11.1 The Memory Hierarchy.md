## 11.1. The Memory Hierarchy

As we explore modern computer storage, a common pattern emerges: devices with higher capacities offer lower performance. Said another way, systems use devices that are fast and devices that store a large amount of data, but no single device does both. This trade-off between performance and capacity is known as the **memory hierarchy**, and [Figure 1](https://diveintosystems.org/book/C11-MemHierarchy/mem_hierarchy.html#FigMemoryHierarchy) depicts the hierarchy visually.

Storage devices similarly trade cost and storage density: faster devices are more expensive, both in terms of bytes per dollar and operational costs (e.g., energy usage). For example, even though caches provide great performance, the cost (and manufacturing challenges) of building a CPU with a large enough cache to forego main memory makes such a design infeasible. Practical systems must utilize a combination of devices to meet the performance and capacity requirements of programs, and a typical system today incorporates most, if not all, of the devices described in [Figure 1](https://diveintosystems.org/book/C11-MemHierarchy/mem_hierarchy.html#FigMemoryHierarchy).

![In order, from (high performance, high cost, low capacity) to (low performance, low cost, high capacity): registers, cache, main memory, flash disk, traditional disk, and remote secondary storage.](https://diveintosystems.org/book/C11-MemHierarchy/_images/MemoryHierarchy.png)

Figure 1. The memory hierarchy

The reality of the memory hierarchy is unfortunate for programmers, who would prefer to not worry about the performance implications of where their data resides. For example, when declaring an integer _in most applications_, a programmer ideally wouldn’t need to agonize over the differences between data stored in a cache or main memory. Requiring a programmer to micromanage which type of memory each variable occupies would be burdensome, although it may occasionally be worth the effort for certain small, performance-critical sections of code.

Note that [Figure 1](https://diveintosystems.org/book/C11-MemHierarchy/mem_hierarchy.html#FigMemoryHierarchy) categorizes _cache_ as single entity, but most systems contain multiple levels of caches that form their own smaller hierarchy. For example, CPUs commonly incorporate a very small and fast _level one_ (L1) cache, which sits relatively close to the ALU, and a larger and slower _level two_ (L2) cache that resides farther away. Many multicore CPUs also share data between cores in a larger _level three_ (L3) cache. While the differences between the cache levels may matter to performance-conscious applications, this book considers just a single level of caching for simplicity.

Though this chapter primarily focuses on data movement between registers, CPU caches, and main memory, the [next section](https://diveintosystems.org/book/C11-MemHierarchy/devices.html#_storage_devices) characterizes common storage devices across the memory hierarchy. We examine disks and their role in the bigger picture of memory management later, in Chapter 13’s discussion of [virtual memory](https://diveintosystems.org/book/C13-OS/vm.html#_virtual_memory).